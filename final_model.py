# -*- coding: utf-8 -*-
"""Employee-Attrition-Predict (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rMHD68i_ZS3gWRviNxbzS15jMSb96cjZ

# Data Collecting
"""

import cml.data_v1 as cmldata
import cdsw

CONNECTION_NAME = "btpn-poc-lake"
conn = cmldata.get_connection(CONNECTION_NAME)
spark = conn.get_spark_session()

attrition = spark.sql(
    """
    SELECT * FROM cml_datasources.hr_ext_employee_attrition a
    """
)

attrition.printSchema()

df = attrition.toPandas()

df.head()

"""# Data Preprocessing"""

# Import library 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_auc_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import explained_variance_score

df.drop(0, axis=0, inplace=True)
df.head()

# display data summary
df.head()

# Showing the sum of rows and columns
df.shape

# showing feature name
df.columns

# showing the data types
df.dtypes

# showing descriptive statistic
df.describe()

# checking duplicates data
df.duplicated().sum()

# checking missing values
df.isnull().sum()

# checking data information summary
df.info()

#Dropping Monthly and Daily Rate
df = df.drop(['monthlyrate', 'dailyrate', 'hourlyrate', 'employeecount', 'stockoptionlevel'], axis=1)

# droping feature single category and id
df = df.drop(['over18', 'employeenumber', 'standardhours'], axis=1)

# Transforming data categorical into numerical

for col in df.columns:
  if df[col].dtype == 'object':
    df[col] = df[col].astype('category')
    df[col] = df[col].cat.codes

# import library imblearn for sampling data

!pip install imblearn
from imblearn import under_sampling, over_sampling
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# make varible X for data feature and Y for label feature
X = df.drop(['attrition'],axis=1)
y = df['attrition']

#apply SelectKBest class to extract scoring feature
# RFE 
bestfeatures = SelectKBest(score_func=chi2, k=15)
fit = bestfeatures.fit(X,y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
print(featureScores.nlargest(15,'Score'))  #print 32 best features

# Drop weakness feature base on score

X = X.drop(['trainingtimeslastyear', 'worklifebalance', 'department',
       'environmentsatisfaction', 'educationfield', 'gender',
       'relationshipsatisfaction', 'percentsalaryhike', 'jobinvolvement',
       'performancerating', 'businesstravel'], axis=1)

# sampling dataset using oversample
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
print(sorted(Counter(y_resampled).items()), y_resampled.shape)

# split dataset 80% for train and 20% for test
X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.2,random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# Model Building"""

# Function to cross validation scoring
# cross validation parameters

def cross_val(X_train, y_train, model):
    # Applying k-Fold Cross Validation
    from sklearn.model_selection import cross_val_score
    accuracies = cross_val_score(estimator=model, X = X_train, y = y_train, cv = 10)
    return accuracies.mean()

"""## Random Forest"""

## Random Forest Classification

model_RF = RandomForestClassifier(n_estimators = 10, criterion='entropy', max_depth=10, random_state=42)
model_RF.fit(X_train.values, y_train)

# import time library for importance feature

import time
import numpy as np

start_time = time.time()
importances = model_RF.feature_importances_
std = np.std([tree.feature_importances_ for tree in model_RF.estimators_], axis=0)
elapsed_time = time.time() - start_time

print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

# visualize importance feature

forest_importances = pd.Series(importances, index=X_train.iloc[0].index)
forest_importances = forest_importances.sort_values(ascending=False)

fig, ax = plt.subplots(figsize=(16, 6))
forest_importances.plot.bar(yerr=std, ax=ax, color='#7978FF')
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

# model Random Forest

pred_val = model_RF.predict(X_train.values)
accuracy_score(y_train,pred_val)

# Random Forest test

model_RF.score(X_test.values, y_test)

# Random Forest Cross Validation

predictions = model_RF.predict(X_test.values)
random_cross = cross_val(X_train.values, y_train, model_RF)

print('Random Forest Performance on the test set: Cross Validation Score = %0.4f' % random_cross)

"""### Model Evaluation"""

# model evaluation
print(classification_report(y_test,predictions))

# PRINTING THE CONFUSION MATRIX

from sklearn.metrics import confusion_matrix

confusion_matrix = confusion_matrix(y_test, predictions)
ax = sns.heatmap(confusion_matrix, annot=True, cmap='Purples')

ax.set_title('Confusion Matrix\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

ax.xaxis.set_ticklabels(['True','False'])
ax.yaxis.set_ticklabels(['True','False'])

plt.show()

"""### Save Model into Pickle"""

import pickle

filename = 'model_RF.pkl'
pickle.dump(model_RF, open(filename, 'wb'))
cdsw.track_file(filename)